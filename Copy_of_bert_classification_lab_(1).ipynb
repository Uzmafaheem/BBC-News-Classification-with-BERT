{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmafaheem/BBC-News-Classification-with-BERT/blob/main/Copy_of_bert_classification_lab_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65571921",
      "metadata": {
        "id": "65571921"
      },
      "source": [
        "# BBC News Classification with BERT: Lab\n",
        "# Solution\n",
        "\n",
        "## Overview\n",
        "As a junior data scientist at NewsInsight, a media analytics company, you've been tasked with building an automated news categorization system. Your team needs to classify incoming news articles into appropriate categories to help journalists, researchers, and business analysts quickly find relevant information.\n",
        "\n",
        "The company receives thousands of articles daily from various sources. Currently, human editors spend significant time manually categorizing these articles, which is time-consuming and inconsistent. Your manager has asked you to develop a machine learning solution that can automatically categorize news articles into predefined categories (business, entertainment, politics, sport, tech).\n",
        "\n",
        "This project will follow the BERT fine-tuning process you've learned:\n",
        "1. Understanding data and defining requirements\n",
        "2. Selecting and preparing the BERT model\n",
        "3. Data preparation and tokenization\n",
        "4. Model architecture design\n",
        "5. Fine-tuning the model\n",
        "6. Evaluation and refinement\n",
        "\n",
        "Successfully implementing this system will significantly improve workflow efficiency, allowing editors to focus on content quality rather than manual categorization.\n",
        "\n",
        "## Part 1: Environment Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d7a44d",
      "metadata": {
        "id": "d5d7a44d"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed_value=42):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c989fc2",
      "metadata": {
        "id": "2c989fc2"
      },
      "outputs": [],
      "source": [
        "# Load the data from csv file\n",
        "#df = pd.read_csv(\"/content/bbc_news_data.csv\", engine='python', quotechar='\"')\n",
        "df = pd.read_csv(\"/content/bbc_news_data.csv\", sep='\\t', engine='python')\n",
        "#df = pd.read_csv(\"/content/bbc_news_data.csv\", engine='python', quotechar='\"', on_bad_lines='skip')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "4TKjfagSZV2u"
      },
      "id": "4TKjfagSZV2u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8b9a4901",
      "metadata": {
        "id": "8b9a4901"
      },
      "source": [
        "## Part 2: Data Exploration and Preprocessing\n",
        "\n",
        "Explore the dataset, displaying basic information and:\n",
        "- Analyze category distribution\n",
        "- Check text length distribution\n",
        "- Train test split data, use 75-25 split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c59aac",
      "metadata": {
        "id": "83c59aac"
      },
      "outputs": [],
      "source": [
        "# Explore the first 10 rows\n",
        "df.head(10)\n",
        "\n",
        "# Basic info and describe\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "# Category distribution\n",
        "category_counts = df['category'].value_counts()\n",
        "category_counts\n",
        "\n",
        "# Visualize category distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "category_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "plt.title(\"Distribution of News Categories\")\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Text length analysis\n",
        "df['text_length'] = df['content'].apply(lambda x: len(x.split()))\n",
        "print(f\"Average text length: {df['text_length'].mean()}\")\n",
        "print(f\"Min text length: {df['text_length'].min()}\")\n",
        "print(f\"Max text length: {df['text_length'].max()}\")\n",
        "\n",
        "# Visualize text length by category\n",
        "plt.figure(figsize=(8,5))\n",
        "df.boxplot(column='text_length', by='category', grid=False, patch_artist=True)\n",
        "plt.title(\"Text Length Distribution by Category\")\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Text Length (words)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Check for missing values\n",
        "df.isnull().sum()\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Map category labels to integers for classification\n",
        "categories = list(df['category'].unique())\n",
        "category_mapping = {category: i for i, category in enumerate(categories)}\n",
        "df['label'] = df['category'].map(category_mapping)\n",
        "\n",
        "# Rename content column into text for hugging face\n",
        "df.rename(columns={'content': 'text'}, inplace=True)\n",
        "\n",
        "# Split into train, validation, and test sets, make sure to stratify based on category, keep features and target together for now\n",
        "# First, create train+validation and test sets\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.25, random_state=42, stratify=df['category'])\n",
        "# Then split train+validation into separate train and validation sets\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42, stratify=train_val_df['category'])\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "# Ensure categories are distributed properly across splits\n",
        "print(\"\\nCategory distribution in training set:\")\n",
        "print(train_df['category'].value_counts())\n",
        "print(\"\\nCategory distribution in validation set:\")\n",
        "print(val_df['category'].value_counts())\n",
        "print(\"\\nCategory distribution in test set:\")\n",
        "print(test_df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65db3cf5",
      "metadata": {
        "id": "65db3cf5"
      },
      "source": [
        "## Part 3: Choose Your Model Approach\n",
        "You can implement either the TensorFlow approach OR the Hugging Face approach. Delete the one you do not use.\n",
        "\n",
        "### ------ TensorFlow Approach --------\n",
        "Implement BERT with TensorFlow and TensorFlow Hub\n",
        "- Import required libraries\n",
        "- Select and load a BERT model\n",
        "- Create datasets\n",
        " - Build model architecture\n",
        " - Fine-tune the model\n",
        " - Evaluate performance\n",
        " - Create visuals for train and validation data metrics across epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75fa516c",
      "metadata": {
        "id": "75fa516c"
      },
      "outputs": [],
      "source": [
        "# Make sure to set legacy Keras to work with TF Hub BERT before you import\n",
        "os.environ['TF_USE_LEGACY_KERAS']= '1'\n",
        "\n",
        "# Import TensorFlow-specific libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "# Select and load BERT model\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "def create_tf_dataset(texts, labels, batch_size=32, shuffle=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(texts))\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Convert pandas DataFrames to TensorFlow datasets\n",
        "train_dataset = create_tf_dataset(train_df['text'].values, train_df['label'].values)\n",
        "val_dataset = create_tf_dataset(val_df['text'].values, val_df['label'].values, shuffle=False)\n",
        "test_dataset = create_tf_dataset(test_df['text'].values, test_df['label'].values, shuffle=False)\n",
        "\n",
        "# Build the BERT model\n",
        "def build_tf_classifier_model():\n",
        "    # Text input\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "\n",
        "    # Preprocessing layer\n",
        "    preprocessor = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessor')\n",
        "\n",
        "    # BERT encoder - set trainable=True for fine-tuning\n",
        "    encoder_inputs = preprocessor(text_input)\n",
        "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "    outputs = encoder(encoder_inputs)\n",
        "\n",
        "    # Use the pooled output for classification\n",
        "    net = outputs['pooled_output']\n",
        "\n",
        "    # Add dropout for regularization\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "\n",
        "    # Add classification layer (for 5 categories)\n",
        "    net = tf.keras.layers.Dense(5, activation= None, name='classifier')(net)\n",
        "\n",
        "    # Create model\n",
        "    model = tf.keras.Model(text_input, net)\n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "tf_classifier_model = build_tf_classifier_model()\n",
        "tf_classifier_model.summary()\n",
        "\n",
        "# Compile the model\n",
        "# Using sparse categorical crossentropy since our labels are integers\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Accuracy as metric\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')]\n",
        "\n",
        "\n",
        "# Set up learning rate and optimizer\n",
        "init_lr = .0005\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=init_lr)\n",
        "\n",
        "# Compile the model\n",
        "tf_classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "# Set up early stopping callback based on validation accuracy\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True, verbose=1)\n",
        "\n",
        "\n",
        "# Train the model for 5 epochs (not enough epochs most likely but to save on time)\n",
        "print('Fine-tuning BERT model...')\n",
        "history = tf_classifier_model.fit(train_dataset, epochs=5, validation_data=val_dataset, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2ebc4e2",
      "metadata": {
        "id": "d2ebc4e2"
      },
      "outputs": [],
      "source": [
        "# Evaluate model on testing data\n",
        "test_loss, test_accuracy = tf_classifier_model.evaluate(test_dataset)\n",
        "print(f'Test accuracy (TensorFlow): {test_accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87c2f5bf",
      "metadata": {
        "id": "87c2f5bf"
      },
      "source": [
        "### ------- Hugging Face Approach -------\n",
        "Implement BERT with Hugging Face Transformers\n",
        "- Import required libraries\n",
        "- Select and load a BERT model\n",
        "- Tokenize data\n",
        "- Create datasets\n",
        "- Fine-tune the model\n",
        "- Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec52f9a2",
      "metadata": {
        "id": "ec52f9a2"
      },
      "outputs": [],
      "source": [
        "# Import Hugging Face libraries\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Select and load tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Convert pandas DataFrames to Hugging Face datasets\n",
        "train_dataset_hf = Dataset.from_pandas(train_df)\n",
        "val_dataset_hf = Dataset.from_pandas(val_df)\n",
        "test_dataset_hf = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Tokenize function (use 128 for max length)\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset_hf.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset_hf.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset_hf.map(tokenize_function, batched=True)\n",
        "\n",
        "# Load pre-trained model with classification head\n",
        "model_hf = BertForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
        "# Set BERT encoder layers to not train\n",
        "for param in model_hf.bert.parameters():\n",
        "    param.requires_grad = True  # Set to False if you want to freeze the encoder\n",
        "\n",
        "\n",
        "# Define metrics computation function the returns a dictionary of scores - include at least accuracy\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\", # Set save strategy to epoch to match eval strategy\n",
        "    learning_rate=5e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"  # disables W&B logging\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_hf,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "print('Fine-tuning BERT model with Hugging Face...')\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate n save\n",
        "trainer.save_model(\"./fine_tuned_bert_hf\")\n",
        "\n",
        "# Evaluate on test dataset\n",
        "\n",
        "print('\\nEvaluating on test data...')\n",
        "test_results = trainer.evaluate(eval_dataset=tokenized_test)\n",
        "print(f\"Test Results: {test_results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16ed0719",
      "metadata": {
        "id": "16ed0719"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate(eval_dataset=tokenized_test)\n",
        "print(f\"Hugging Face Model Results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7b84ef",
      "metadata": {
        "id": "5f7b84ef"
      },
      "source": [
        "## Part 4: Model Analysis and Inference\n",
        "Analyze model performance on testing data\n",
        "- Create confusion matrix visualization\n",
        "- Analyze misclassifications\n",
        "- Identify strengths and weaknesses\n",
        "\n",
        "### Tensorflow Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d5691ba",
      "metadata": {
        "id": "8d5691ba"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the test set\n",
        "test_predictions = tf_classifier_model.predict(test_dataset)\n",
        "y_pred = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Get true labels from testing data\n",
        "y_true = [labels.numpy() for _, labels in test_dataset.unbatch()]\n",
        "\n",
        "# Create and visualize the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=categories)\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title('Confusion Matrix for BBC News Classification (TensorFlow)')\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# Create classification report\n",
        "report = classification_report(y_true, y_pred, target_names=categories, output_dict=True)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=categories))\n",
        "\n",
        "# Identify strengths and weaknesses\n",
        "print(\"\\nModel Strengths and Weaknesses:\")\n",
        "\n",
        "# Calculate per-class metrics (use classification report)\n",
        "per_class_metrics = {}\n",
        "for category in categories:\n",
        "    per_class_metrics[category] = report[category]\n",
        "\n",
        "# Find best and worst performing categories\n",
        "best_category = max(categories, key=lambda x: per_class_metrics[x]['f1-score'])\n",
        "worst_category = min(categories, key=lambda x: per_class_metrics[x]['f1-score'])\n",
        "\n",
        "print(f\"\\nStrengths:\")\n",
        "print(f\"- Overall accuracy: {report['accuracy']:.4f}\")\n",
        "print(f\"- Best performing category: {best_category}\")\n",
        "print(f\"\\nWeaknesses:\")\n",
        "print(f\"- Worst performing category: {worst_category}\")\n",
        "\n",
        "# Visualize per-class performance\n",
        "plt.figure(figsize=(12, 6))\n",
        "categories_indices = range(len(categories))\n",
        "width = 0.25\n",
        "\n",
        "plt.bar([i - width for i in categories_indices],\n",
        "        [per_class_metrics[cat]['precision'] for cat in categories],\n",
        "        width=width, label='Precision')\n",
        "plt.bar(categories_indices,\n",
        "        [per_class_metrics[cat]['recall'] for cat in categories],\n",
        "        width=width, label='Recall')\n",
        "plt.bar([i + width for i in categories_indices],\n",
        "        [per_class_metrics[cat]['f1-score'] for cat in categories],\n",
        "        width=width, label='F1-Score')\n",
        "\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Performance Metrics by Category')\n",
        "plt.xticks(categories_indices, categories, rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9115cc66",
      "metadata": {
        "id": "9115cc66"
      },
      "outputs": [],
      "source": [
        "# Create a function for model inference on new articles\n",
        "def predict_article_category(text, model=tf_classifier_model):\n",
        "    \"\"\"\n",
        "    Predict the category of a news article using the fine-tuned model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text of the news article\n",
        "        model: The fine-tuned TensorFlow model\n",
        "\n",
        "    Returns:\n",
        "        dict: Prediction results including category and confidence scores\n",
        "    \"\"\"\n",
        "    # Make prediction\n",
        "    prediction = model.predict([text])[0]\n",
        "\n",
        "    # Get the predicted category and confidence\n",
        "    predicted_class_id = np.argmax(prediction)\n",
        "    predicted_category = categories[predicted_class_id]\n",
        "    confidence = float(prediction[predicted_class_id])\n",
        "\n",
        "    # Get confidence for all categories\n",
        "    category_confidences = {categories[i]: float(prediction[i]) for i in range(len(categories))}\n",
        "\n",
        "    # Sort categories by confidence (descending)\n",
        "    sorted_categories = sorted(category_confidences.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return {\n",
        "        'text': text[:100] + '...' if len(text) > 100 else text,\n",
        "        'predicted_category': predicted_category,\n",
        "        'confidence': confidence,\n",
        "        'all_confidences': sorted_categories\n",
        "    }\n",
        "\n",
        "# Test the inference function with example articles\n",
        "sample_articles = [\n",
        "    \"The tech giant announced the release of their new smartphone that features advanced AI capabilities and improved battery life. The product will be available in stores next month.\",\n",
        "    \"The football team secured their victory in the final minutes with a spectacular goal. The win puts them at the top of the league table.\",\n",
        "    \"Stock markets plummeted following the central bank's announcement of interest rate increases. Investors are concerned about the impact on economic growth.\",\n",
        "    \"The new film starring the award-winning actress has received critical acclaim at the international film festival. Critics praised the innovative cinematography.\",\n",
        "    \"The government announced new policies regarding digital privacy and data protection. Opposition parties have criticized the measures as inadequate.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting inference on sample articles:\")\n",
        "for i, article in enumerate(sample_articles):\n",
        "    result = predict_article_category(article)\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Predicted category: {result['predicted_category']} (confidence: {result['confidence']:.4f})\")\n",
        "    print(\"All category confidences:\")\n",
        "    for category, conf in result['all_confidences']:\n",
        "        print(f\"  - {category}: {conf:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc5d541f",
      "metadata": {
        "id": "fc5d541f"
      },
      "source": [
        "### Hugging Face Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff0465b",
      "metadata": {
        "id": "9ff0465b"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the test set\n",
        "def get_predictions(trainer, dataset):\n",
        "    # Run predictions with Hugging Face Trainer\n",
        "    raw_predictions = trainer.predict(dataset)\n",
        "\n",
        "    # Extract predictions and labels\n",
        "    predictions = np.argmax(raw_predictions.predictions, axis=1)\n",
        "    labels = dataset['label'] # Corrected column name\n",
        "\n",
        "    return predictions, labels\n",
        "\n",
        "y_pred, y_true = get_predictions(trainer, tokenized_test)\n",
        "\n",
        "# Create and visualize the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=categories)\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title('Confusion Matrix for BBC News Classification (Hugging Face)')\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# Create classification report\n",
        "report = classification_report(y_true, y_pred, target_names=categories, output_dict=True)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=categories))\n",
        "\n",
        "\n",
        "# Identify strengths and weaknesses\n",
        "print(\"\\nModel Strengths and Weaknesses:\")\n",
        "\n",
        "# Calculate per-class metrics (use classification report)\n",
        "per_class_metrics = {}\n",
        "for category in categories: # Corrected population of per_class_metrics\n",
        "    per_class_metrics[category] = report[category]\n",
        "\n",
        "\n",
        "# Find best and worst performing categories\n",
        "best_category = max(per_class_metrics, key=lambda x: per_class_metrics[x]['f1-score'])\n",
        "worst_category = min(per_class_metrics, key=lambda x: per_class_metrics[x]['f1-score'])\n",
        "\n",
        "print(f\"\\nStrengths:\")\n",
        "print(f\"- Overall accuracy: {report['accuracy']:.4f}\")\n",
        "print(f\"- Best performing category: {best_category}\") # Corrected to use category name directly\n",
        "print(f\"\\nWeaknesses:\")\n",
        "print(f\"- Worst performing category: {worst_category}\") # Corrected to use category name directly\n",
        "\n",
        "# Visualize per-class performance\n",
        "plt.figure(figsize=(12, 6))\n",
        "categories_indices = range(len(categories))\n",
        "width = 0.25\n",
        "\n",
        "plt.bar([i - width for i in categories_indices],\n",
        "        [per_class_metrics[cat]['precision'] for cat in categories],\n",
        "        width=width, label='Precision')\n",
        "plt.bar(categories_indices,\n",
        "        [per_class_metrics[cat]['recall'] for cat in categories],\n",
        "        width=width, label='Recall')\n",
        "plt.bar([i + width for i in categories_indices],\n",
        "        [per_class_metrics[cat]['f1-score'] for cat in categories],\n",
        "        width=width, label='F1-Score')\n",
        "\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Performance Metrics by Category')\n",
        "plt.xticks(categories_indices, categories, rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "759f2b7a",
      "metadata": {
        "id": "759f2b7a"
      },
      "outputs": [],
      "source": [
        "# Create a function for model inference on new articles\n",
        "def predict_article_category(text, model=model_hf, tokenizer=tokenizer):\n",
        "    \"\"\"\n",
        "    Predict the category of a news article using the fine-tuned model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text of the news article\n",
        "        model: The fine-tuned Hugging Face model\n",
        "        tokenizer: The tokenizer for the model\n",
        "\n",
        "    Returns:\n",
        "        dict: Prediction results including category and confidence scores\n",
        "    \"\"\"\n",
        "    # Tokenize inputs and move to the same device as the model\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    device = model.device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "        probs = probs.squeeze().tolist()\n",
        "        predicted_class_id = torch.argmax(logits, dim=1).item()\n",
        "        predicted_category = categories[predicted_class_id]\n",
        "        confidence = float(probs[predicted_class_id])\n",
        "\n",
        "    # Get confidence for all categories\n",
        "    category_confidences = {categories[i]: float(probs[i]) for i in range(len(categories))}\n",
        "    # Convert to numpy for easier handling\n",
        "    probs = np.array(probs)\n",
        "\n",
        "    # Get the predicted category and confidence\n",
        "    predicted_class_id = np.argmax(probs)\n",
        "    predicted_category = categories[predicted_class_id]\n",
        "    confidence = float(probs[predicted_class_id])\n",
        "\n",
        "    # Get confidence for all categories\n",
        "    category_confidences = {categories[i]: float(probs[i]) for i in range(len(categories))}\n",
        "\n",
        "    # Sort categories by confidence (descending)\n",
        "    sorted_categories = sorted(category_confidences.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return {\n",
        "        'text': text[:100] + '...' if len(text) > 100 else text,\n",
        "        'predicted_category': predicted_category,\n",
        "        'confidence': confidence,\n",
        "        'all_confidences': sorted_categories\n",
        "    }\n",
        "\n",
        "# Test the inference function with example articles\n",
        "sample_articles = [\n",
        "    \"The tech giant announced the release of their new smartphone that features advanced AI capabilities and improved battery life. The product will be available in stores next month.\",\n",
        "    \"The football team secured their victory in the final minutes with a spectacular goal. The win puts them at the top of the league table.\",\n",
        "    \"Stock markets plummeted following the central bank's announcement of interest rate increases. Investors are concerned about the impact on economic growth.\",\n",
        "    \"The new film starring the award-winning actress has received critical acclaim at the international film festival. Critics praised the innovative cinematography.\",\n",
        "    \"The government announced new policies regarding digital privacy and data protection. Opposition parties have criticized the measures as inadequate.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting inference on sample articles:\")\n",
        "for i, article in enumerate(sample_articles):\n",
        "    result = predict_article_category(article)\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Predicted category: {result['predicted_category']} (confidence: {result['confidence']:.4f})\")\n",
        "    print(\"All category confidences:\")\n",
        "    for category, conf in result['all_confidences']:\n",
        "        print(f\"  - {category}: {conf:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1️⃣ Model Performance Metrics\n",
        "- From your earlier training and evaluation:\n",
        "   - Final test accuracy:  0.964\n",
        "- From hugging Face Model\n",
        "   - Final test accuracy: 0.6140\n",
        "- From TensorFlow Model\n",
        "    - Overall accuracy: 0.9641\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "F_3dTl56WPM5"
      },
      "id": "F_3dTl56WPM5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Category          | Observations                                                                                                                  |\n",
        "| ----------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Business**      | Frequently predicted correctly; sometimes confused with **Tech** due to overlapping vocabulary (e.g., “AI”, “stock markets”). |\n",
        "| **Tech**          | Easier to confuse with Business; moderately accurate.                                                                         |\n",
        "| **Politics**      | Sometimes confused with **Business** or **Tech** in policy-related articles.                                                  |\n",
        "| **Entertainment** | Usually predicted with high confidence when mentions of movies, actors, awards appear.                                        |\n",
        "| **Sport**         | Strong predictions for clearly sports-related words like “football”, “goal”, “victory”.                                       |\n"
      ],
      "metadata": {
        "id": "bb9MCazCVpuv"
      },
      "id": "bb9MCazCVpuv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3️⃣ Challenges Encountered\n",
        "\n",
        "- Vocabulary overlap: Business and Tech articles often share words like “AI”, “investment”, “digital”, making it harder for the model to differentiate.\n",
        "\n",
        "- Limited context in short text samples: Short summaries or titles may not provide enough context for clear classification.\n",
        "\n",
        "- Imbalanced predictions: Some categories like Business appeared more frequently in predictions, which may indicate slight class imbalance in the dataset or overfitting.\n",
        "\n",
        "- Model confidence repetition: Several samples had the exact same confidence scores (e.g., business: 0.6006), which may indicate either the softmax outputs are not fully differentiated or batch-level inference artifacts."
      ],
      "metadata": {
        "id": "d9iANlmoVxKx"
      },
      "id": "d9iANlmoVxKx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4️⃣ Recommendations for Improvement\n",
        "\n",
        "## Data Improvements:\n",
        "- Increase dataset size for underrepresented categories.\n",
        "- Use data augmentation (e.g., paraphrasing) to improve generalization.\n",
        "\n",
        "## Model Enhancements:\n",
        "- Experiment with larger BERT models (bert-base → bert-large) or domain-specific models (e.g., FinBERT for financial articles).\n",
        "- Fine-tune longer and adjust learning rate schedules.\n",
        "- Freeze fewer layers for better fine-tuning if you have enough data.\n",
        "\n",
        "## Post-processing:\n",
        "- Use label smoothing or confidence thresholding for uncertain predictions.\n",
        "\n",
        "- Ensemble models (e.g., BERT + RoBERTa) for more robust classification.\n",
        "\n",
        "## Evaluation Enhancements:\n",
        "\n",
        "- Generate confusion matrices to see which categories are commonly misclassified.\n",
        "\n",
        "- Track per-class metrics to monitor weak categories and prioritize improvements."
      ],
      "metadata": {
        "id": "dxluig1_V-XT"
      },
      "id": "dxluig1_V-XT"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}